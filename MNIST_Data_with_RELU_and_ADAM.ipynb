{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST Data with RELU and ADAM.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "e9R4tmiJPzoY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IbdTAkuOP_tz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Resetting the graph\n",
        "tf.reset_default_graph()\n",
        "tf.set_random_seed(34)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OVxR8Fv2QQJT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "41139bea-32cc-4241-cf3f-6e97fdbd14ce"
      },
      "cell_type": "code",
      "source": [
        "#Let us import the training and test data set from MNIST data\n",
        "(trainX, trainy),(testX, testy) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U0Eq7nS6Q6LP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a31c537b-6b71-4987-e8f8-afa957182cfd"
      },
      "cell_type": "code",
      "source": [
        "#Let us print the shape of training data set and testing data set\n",
        "print(\"Train X shape, Test X shape, train y shape, test y shape\", trainX.shape, testX.shape, trainy.shape, testy.shape)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train X shape, Test X shape, train y shape, test y shape (60000, 28, 28) (10000, 28, 28) (60000,) (10000,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uydm18KhRZP6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let us convert the data set Y values to categorical\n",
        "trainy = tf.keras.utils.to_categorical(trainy, num_classes=10)\n",
        "testy = tf.keras.utils.to_categorical(testy, num_classes=10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KaIxzsA-U4lH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below three cells help  us initialize and build the graph. \n",
        "1. Initialize the Sequential model\n",
        "2. Add a Reshape Layer and pass on the input shape and to which size should we reshape the input data\n",
        "3. Add a batch normalization layer to calculate the below\n",
        "  1. Batch mean\n",
        "  2. Batch variance\n",
        "  3. Batch norm\n",
        "  4. Scale and shift the batch nor"
      ]
    },
    {
      "metadata": {
        "id": "LH49Q4WzR11m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let us initialize a sequential model\n",
        "model = tf.keras.models.Sequential()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4QCxStM1SEvk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let us start adding layers to the model.\n",
        "#The below layer will help in reshaping the input data from a shape of (28,28,) into (784,).\n",
        "#Here 784 is nothing but 28*28. So we are reducing a three dimensional image data to two dimensions.\n",
        "model.add(tf.keras.layers.Reshape((784,), input_shape=(28,28,)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQgmOB44UQ-A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let us add batch normalization in the input layer\n",
        "model.add(tf.keras.layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "irljiwCBVvtH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Let us start adding more hidden layers to the model. We will follow the below steps\n",
        "1. Add a Dense hidden layer with activation function as RELU.\n",
        "  1. We would provide the total number of neurons to the Dense layer, activation function and if needed, a name to the layer.\n",
        "2. Add a batch normalization layer. This will help in normalization of weights and biases which get altered as a result of back propagation. \n",
        "  1. It is suggested that batch normalization can be employed after each hidden layer, so this takes care of the input weights and biases which will be passed on as input to the next hidden layer.\n",
        " 3. The output layer will have total number of neurons =  the total number of categories, here it is 10 and the activation function would be softmax"
      ]
    },
    {
      "metadata": {
        "id": "aCq4ab-LU0kY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(units=200, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xR9yOl0eYc_J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.add(tf.keras.layers.Dense(units=100, activation='relu'))\n",
        "model.add(tf.keras.layers.BatchNormalization())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kaWQM6CdYmhx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#Let us add output layer\n",
        "model.add(tf.keras.layers.Dense(units=10, activation='softmax'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6TX3ckDqZbRt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The below code will compile the model. We will be passing loss function as categorical_crossentropy, optimizer as adam and the metrics will be accuracy"
      ]
    },
    {
      "metadata": {
        "id": "Iaw2OG6KZYMO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lom75yJ9Z6fb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1176
        },
        "outputId": "9fb3a5fd-d510-4a19-e9de-a3e4762a8d2c"
      },
      "cell_type": "code",
      "source": [
        "#Let us fit the training data to the model. This will execute the model\n",
        "model.fit(trainX, trainy, validation_data=(testX, testy), batch_size=32, epochs=30)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/30\n",
            "60000/60000 [==============================] - 13s 221us/step - loss: 0.2293 - acc: 0.9286 - val_loss: 0.1681 - val_acc: 0.9629\n",
            "Epoch 2/30\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.1141 - acc: 0.9644 - val_loss: 0.1522 - val_acc: 0.9672\n",
            "Epoch 3/30\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0858 - acc: 0.9726 - val_loss: 0.1535 - val_acc: 0.9699\n",
            "Epoch 4/30\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.0728 - acc: 0.9770 - val_loss: 0.1621 - val_acc: 0.9688\n",
            "Epoch 5/30\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0586 - acc: 0.9812 - val_loss: 0.1338 - val_acc: 0.9735\n",
            "Epoch 6/30\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0536 - acc: 0.9821 - val_loss: 0.1332 - val_acc: 0.9738\n",
            "Epoch 7/30\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0459 - acc: 0.9848 - val_loss: 0.1398 - val_acc: 0.9734\n",
            "Epoch 8/30\n",
            "60000/60000 [==============================] - 14s 230us/step - loss: 0.0420 - acc: 0.9857 - val_loss: 0.1544 - val_acc: 0.9713\n",
            "Epoch 9/30\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0361 - acc: 0.9880 - val_loss: 0.1627 - val_acc: 0.9725\n",
            "Epoch 10/30\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0339 - acc: 0.9886 - val_loss: 0.1639 - val_acc: 0.9728\n",
            "Epoch 11/30\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0319 - acc: 0.9894 - val_loss: 0.1668 - val_acc: 0.9742\n",
            "Epoch 12/30\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.0294 - acc: 0.9902 - val_loss: 0.1750 - val_acc: 0.9711\n",
            "Epoch 13/30\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0274 - acc: 0.9910 - val_loss: 0.1983 - val_acc: 0.9708\n",
            "Epoch 14/30\n",
            "60000/60000 [==============================] - 12s 204us/step - loss: 0.0241 - acc: 0.9922 - val_loss: 0.1630 - val_acc: 0.9734\n",
            "Epoch 15/30\n",
            "60000/60000 [==============================] - 12s 202us/step - loss: 0.0245 - acc: 0.9919 - val_loss: 0.1711 - val_acc: 0.9729\n",
            "Epoch 16/30\n",
            "60000/60000 [==============================] - 12s 207us/step - loss: 0.0241 - acc: 0.9920 - val_loss: 0.1971 - val_acc: 0.9711\n",
            "Epoch 17/30\n",
            "60000/60000 [==============================] - 12s 201us/step - loss: 0.0208 - acc: 0.9929 - val_loss: 0.1824 - val_acc: 0.9716\n",
            "Epoch 18/30\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0209 - acc: 0.9930 - val_loss: 0.1662 - val_acc: 0.9737\n",
            "Epoch 19/30\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0203 - acc: 0.9933 - val_loss: 0.1588 - val_acc: 0.9743\n",
            "Epoch 20/30\n",
            "60000/60000 [==============================] - 12s 205us/step - loss: 0.0195 - acc: 0.9933 - val_loss: 0.2097 - val_acc: 0.9702\n",
            "Epoch 21/30\n",
            "60000/60000 [==============================] - 12s 203us/step - loss: 0.0172 - acc: 0.9940 - val_loss: 0.1796 - val_acc: 0.9718\n",
            "Epoch 22/30\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0182 - acc: 0.9936 - val_loss: 0.1809 - val_acc: 0.9732\n",
            "Epoch 23/30\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 0.0158 - acc: 0.9947 - val_loss: 0.1990 - val_acc: 0.9719\n",
            "Epoch 24/30\n",
            "60000/60000 [==============================] - 13s 211us/step - loss: 0.0172 - acc: 0.9940 - val_loss: 0.2124 - val_acc: 0.9717\n",
            "Epoch 25/30\n",
            "60000/60000 [==============================] - 12s 208us/step - loss: 0.0153 - acc: 0.9949 - val_loss: 0.1737 - val_acc: 0.9747\n",
            "Epoch 26/30\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0152 - acc: 0.9947 - val_loss: 0.1785 - val_acc: 0.9730\n",
            "Epoch 27/30\n",
            "60000/60000 [==============================] - 13s 210us/step - loss: 0.0148 - acc: 0.9950 - val_loss: 0.1738 - val_acc: 0.9748\n",
            "Epoch 28/30\n",
            "60000/60000 [==============================] - 13s 212us/step - loss: 0.0136 - acc: 0.9954 - val_loss: 0.1898 - val_acc: 0.9734\n",
            "Epoch 29/30\n",
            "60000/60000 [==============================] - 13s 209us/step - loss: 0.0146 - acc: 0.9952 - val_loss: 0.1864 - val_acc: 0.9728\n",
            "Epoch 30/30\n",
            "60000/60000 [==============================] - 12s 206us/step - loss: 0.0137 - acc: 0.9953 - val_loss: 0.1917 - val_acc: 0.9712\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff0ba20e4e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    }
  ]
}